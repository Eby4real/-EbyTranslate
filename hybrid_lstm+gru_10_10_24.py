# -*- coding: utf-8 -*-
"""Hybrid_lstm+gru 10.10.24.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10xj1aOAPvqQ8Qdkd6ZlM84Qdc_7oBzeK
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import tensorflow as tf
import os
import json

from tensorflow.keras.preprocessing.text import Tokenizer
from collections import Counter
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Model, Sequential, model_from_json
from tensorflow.keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, LSTM, Dropout, Embedding
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import sparse_categorical_crossentropy
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from wordcloud import WordCloud

# Function to load and preprocess the data
def load_doc(url):
    df = pd.read_csv(url, delimiter="\t", header=None)
    return df

# Load the dataset
doc = load_doc("./fra.txt")
doc = doc.iloc[:, :2]
doc.columns = ['english', 'french']
doc['english'] = doc['english'].astype(str)
doc['french'] = doc['french'].astype(str)

eng = doc['english']
fr = doc['french']

def tokenize(x):
    tokenizer = Tokenizer()
    tokenizer.fit_on_texts(x)
    return tokenizer.texts_to_sequences(x), tokenizer

def pad(x, length=None):
    if length is None:
        length = max([len(sentence) for sentence in x])
    return pad_sequences(x, maxlen=55, padding='post')

def preprocess(x, y):
    preprocess_x, x_tk = tokenize(x)
    preprocess_y, y_tk = tokenize(y)

    preprocess_x = pad(preprocess_x)
    preprocess_y = pad(preprocess_y)

    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)

    return preprocess_x, preprocess_y, x_tk, y_tk

# Preprocess the sentences
preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer = preprocess(eng, fr)
tmp_x = preproc_french_sentences  # Assuming tmp_x is meant to be preproc_french_sentences
# Model parameters
max_english_sequence_length = preproc_english_sentences.shape[1]
max_french_sequence_length = preproc_french_sentences.shape[1]
english_vocab_size = len(english_tokenizer.word_index) + 1
french_vocab_size = len(french_tokenizer.word_index)

def logits_to_text(logits, tokenizer):
    index_to_words = {id: word for word, id in tokenizer.word_index.items()}
    index_to_words[0] = '<PAD>'
    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])

os.makedirs('model', exist_ok=True)

with open("model/english_tokenizer.pkl", 'wb') as handle:
    pickle.dump(english_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

with open("model/french_tokenizer.pkl", 'wb') as handle:
    pickle.dump(french_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

max_english_sequence_length = preproc_english_sentences.shape[1]
max_french_sequence_length = preproc_french_sentences.shape[1]
english_vocab_size = len(english_tokenizer.word_index) + 1
french_vocab_size = len(french_tokenizer.word_index) + 1

print("Max English sentence length:", max_english_sequence_length)
print("Max French sentence length:", max_french_sequence_length)
print("English vocabulary size:", english_vocab_size)
print("French vocabulary size:", french_vocab_size)

# Hybrid LSTM-GRU Model
def hybrid_lstm_gru_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):
    learning_rate = 0.001
    model = Sequential()
    model.add(Input(shape=input_shape))
    model.add(Embedding(input_dim=english_vocab_size, output_dim=256))
    model.add(Bidirectional(LSTM(128, return_sequences=True)))
    model.add(Bidirectional(GRU(128, return_sequences=True)))
    model.add(TimeDistributed(Dense(1024, activation='relu')))
    model.add(Dropout(0.5))
    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))
    model.compile(loss=sparse_categorical_crossentropy, optimizer=Adam(learning_rate), metrics=['accuracy'])
    return model

# Setting up the input shape for the hybrid model
input_shape = (max_english_sequence_length,)

# Create and summarize the model
model_hybrid = hybrid_lstm_gru_model(input_shape, max_french_sequence_length, english_vocab_size, french_vocab_size)
model_hybrid.summary()

# Callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1)
model_checkpoint_hybrid = ModelCheckpoint(filepath='model/hybrid_model.keras', save_best_only=True, monitor='val_loss', verbose=1)

# Training the hybrid model
history_hybrid = model_hybrid.fit(preproc_english_sentences, preproc_french_sentences, batch_size=64, epochs=5, validation_split=0.2, callbacks=[early_stopping, model_checkpoint_hybrid])

# Save the trained model
model_hybrid.save('model/hybrid_model.h5')
# Evaluate the model
loss_hybrid, accuracy_hybrid = model_hybrid.evaluate(preproc_english_sentences, preproc_french_sentences)
print("Hybrid Model Loss:", loss_hybrid)
print("Hybrid Model Accuracy:", accuracy_hybrid)

# Plot accuracy and loss
plt.figure(figsize=(6, 4))
plt.plot(history_hybrid.history['accuracy'], label='Hybrid Model Accuracy')
plt.plot(history_hybrid.history['val_accuracy'], label='Hybrid Model Validation Accuracy')
plt.title('Hybrid Model Accuracy vs. Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

plt.figure(figsize=(6, 4))
plt.plot(history_hybrid.history['loss'], label='Hybrid Model Loss')
plt.plot(history_hybrid.history['val_loss'], label='Hybrid Model Validation Loss')
plt.title('Hybrid Model Loss vs. Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Function to translate English sentences to French
def translate_sentence(sentence, model, english_tokenizer, french_tokenizer, max_french_sequence_length):
    # Preprocess the input sentence
    sequence = english_tokenizer.texts_to_sequences([sentence])
    padded_sequence = pad_sequences(sequence, maxlen=max_english_sequence_length, padding='post')

    # Predict the output
    prediction = model.predict(padded_sequence)
    predicted_sequence = np.argmax(prediction, axis=-1)

    # Convert the predicted sequence to words
    translated_sentence = logits_to_text(predicted_sequence[0], french_tokenizer)
    return translated_sentence

# Example usage
test_sentence = "How are you?"  # You can change this to any sentence you want to translate
translated = translate_sentence(test_sentence, model_hybrid, english_tokenizer, french_tokenizer, max_french_sequence_length)
print(f'Translation of "{test_sentence}": {translated}')

import numpy as np
from keras.preprocessing.sequence import pad_sequences

# Function to translate English sentences to French
def translate_sentence(sentence, model, english_tokenizer, french_tokenizer, max_english_sequence_length, max_french_sequence_length):
    # Preprocess the input sentence
    sequence = english_tokenizer.texts_to_sequences([sentence])
    padded_sequence = pad_sequences(sequence, maxlen=max_english_sequence_length, padding='post')

    # Predict the output (ensure the input is in the correct shape)
    prediction = model.predict(padded_sequence)

    # Check the shape of the prediction output
    print(f"Prediction shape: {prediction.shape}")  # For debugging purposes

    # Get the argmax to get the predicted token indices
    predicted_sequence = np.argmax(prediction, axis=-1)

    # Convert the predicted sequence to words using the French tokenizer
    translated_sentence = logits_to_text(predicted_sequence[0], french_tokenizer)

    return translated_sentence

# Example usage
test_sentence = "How are you?"  # You can change this to any sentence you want to translate
translated = translate_sentence(test_sentence, model_hybrid, english_tokenizer, french_tokenizer, max_english_sequence_length, max_french_sequence_length)
print(f'Translation of "{test_sentence}": {translated}')